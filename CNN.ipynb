{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa893ff5-81cd-414b-a319-6c53ad792952",
   "metadata": {},
   "source": [
    "*******CNN: Convolutional Neural Network(CNN or ConvNet)\n",
    "is a class of deep neural networks which is mostly used to do image recognition, image classification, object detection, etc.*******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e087b12f-31d0-491f-921e-7c810154567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3a7bead-7aa5-467b-bdb9-e3788b877e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afe3a944-add5-4a77-879a-e01ef109a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9efee00f-1d41-4f8c-aec7-3d2858e6a6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7abad-8130-4611-8908-010063f8e388",
   "metadata": {},
   "source": [
    "*****PART-1*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b27bc-d553-46ac-bc71-ac2ffc437bba",
   "metadata": {},
   "source": [
    "*******DATA PROCESSING********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "390d3e51-e7ff-41ab-9eff-12da8cb6699d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 275 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,#Scales the image pixel values to be between 0 and 1\n",
    "                                  shear_range = 0.2,# Applies random shear transformations.\n",
    "                                  zoom_range = 0.2,# Applies random zoom transformations.\n",
    "                                  horizontal_flip = True)#Randomly flips images horizontally.\n",
    "training_set = train_datagen.flow_from_directory(#method to generate batches of augmented data from images stored in a directory. It's \n",
    "    #a convenient way to handle large datasets without having to load all images into memory at once.\n",
    "        r'C:\\Users\\DELL\\Desktop\\datasets\\train',# Specifies the path to the directory containing the images\n",
    "         target_size=(64, 64),#Resizes all images to the specified dimensions (64x64 pixels in your case).\n",
    "        batch_size=32,#Number of images in each batch of data.\n",
    "        class_mode='binary')#indicating that you have binary labels (e.g., 0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85b2a2bc-c997-4b5b-afc7-a93ba175a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "       r'C:\\Users\\DELL\\Desktop\\datasets\\test',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a511664-7502-4312-a959-695e6bb955ab",
   "metadata": {},
   "source": [
    "**************part-2**************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77843420-c187-41de-aa2e-57196a0e3e14",
   "metadata": {},
   "source": [
    "************Building CNN************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5eb9bb35-4817-4886-8dab-a75e210bf67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=tf.keras.models.Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f494f83-1078-4d65-9b02-72ce43e7e1f5",
   "metadata": {},
   "source": [
    "*****Step-1 convolution*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2633245-4cdd-402a-9400-4b28b5df19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation=\"relu\",input_shape=[64,64,3]))\n",
    "  #tf.keras.layers.Conv2D: This creates a 2D convolutional layer. In convolutional neural networks (CNNs), convolutional layers apply convolution operations to the input, extracting features from the input data\n",
    "  #filters=32: This parameter specifies the number of filters (or kernels) in the convolutional layer. Each filter detects different features in the input image. Here, you've set it to 32, meaning there will be 32 filters in this layer.\n",
    "  #kernel_size=3: This parameter defines the size of the convolutional kernels. A kernel of size 3x3 is commonly used. It determines the spatial extent of the convolution window.\n",
    "  #activation=\"relu\": This parameter specifies the activation function applied to the output of this convolutional layer. ReLU (Rectified Linear Unit) is a popular choice for activation functions in deep learning models due to its simplicity and effectiveness in combating the vanishing gradient problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b21cd9-b6ef-41bf-9b34-00e489e621cb",
   "metadata": {},
   "source": [
    "*****step-2 maxpooling*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd9b1c81-b279-4f76-b561-6531453bee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411cdac-d146-4b06-89f4-f6897c587042",
   "metadata": {},
   "source": [
    "******Adding second convolution layer:******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9bbe7ae-a53a-4160-99f6-caf7423012ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation=\"relu\"))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10531bfd-0977-42ac-90b4-5f1c22f02b5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m9,248\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,144</span> (39.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,144\u001b[0m (39.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,144</span> (39.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,144\u001b[0m (39.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd07324b-7aae-4e34-b42d-8322b0104d37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fab13b0-59a3-4b47-b116-c4aad7450afa",
   "metadata": {},
   "source": [
    "*****step-3 Flattening******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "325a2bd8-6bd5-47cd-873a-8a577c255e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())# To pass these features to fully connected layers for classification, they need to be flattened into a one-dimensional vector. That's the role of the flatten layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61385853-c255-49e9-bb5c-04711a6c7d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b2608f1-7e53-404d-a7c6-af4460ad580d",
   "metadata": {},
   "source": [
    "*****step-4 Full connection********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98fdfb32-5532-4304-b992-67db62ba39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128,activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dbf645-73ee-4f5e-8165-c098800f928b",
   "metadata": {},
   "source": [
    "*****step-5 Output layer*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6cab607c-7daf-4076-bc3e-bbd919d5da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))#tf.keras.layers.Dense: This creates a fully connected layer. In a dense layer, every node is connected to every node in the previous layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd2cd1-218a-422a-9c8f-e39b99d824cc",
   "metadata": {},
   "source": [
    "##*************Part-3 Training the CNN*************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c444969-3473-402f-9262-c980900964dd",
   "metadata": {},
   "source": [
    "****compiling the cnn****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87c61a03-b415-43b4-a027-4460dfff23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a137e-d278-4702-b0a7-0997aa4f0713",
   "metadata": {},
   "source": [
    "*****Training the CNN on the training set and evaluating it on the test set:******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1650d9a1-fea2-4cc5-9416-2967af8b5955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 222ms/step - accuracy: 0.6414 - loss: 0.7522 - val_accuracy: 0.6571 - val_loss: 0.6310\n",
      "Epoch 2/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.6227 - loss: 0.6538 - val_accuracy: 0.6571 - val_loss: 0.6435\n",
      "Epoch 3/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - accuracy: 0.6486 - loss: 0.6143 - val_accuracy: 0.6571 - val_loss: 0.6245\n",
      "Epoch 4/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.6664 - loss: 0.5792 - val_accuracy: 0.6857 - val_loss: 0.6210\n",
      "Epoch 5/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.7539 - loss: 0.5524 - val_accuracy: 0.7429 - val_loss: 0.6352\n",
      "Epoch 6/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.7122 - loss: 0.5772 - val_accuracy: 0.7000 - val_loss: 0.6468\n",
      "Epoch 7/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 134ms/step - accuracy: 0.7248 - loss: 0.5502 - val_accuracy: 0.6571 - val_loss: 0.9374\n",
      "Epoch 8/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 143ms/step - accuracy: 0.6939 - loss: 0.5440 - val_accuracy: 0.6429 - val_loss: 0.6676\n",
      "Epoch 9/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.7312 - loss: 0.5246 - val_accuracy: 0.6571 - val_loss: 0.7081\n",
      "Epoch 10/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.7525 - loss: 0.5158 - val_accuracy: 0.6571 - val_loss: 0.9517\n",
      "Epoch 11/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.7235 - loss: 0.5047 - val_accuracy: 0.6571 - val_loss: 0.6968\n",
      "Epoch 12/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.7626 - loss: 0.5317 - val_accuracy: 0.6429 - val_loss: 0.6869\n",
      "Epoch 13/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.7629 - loss: 0.4760 - val_accuracy: 0.6857 - val_loss: 0.8074\n",
      "Epoch 14/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.8036 - loss: 0.4284 - val_accuracy: 0.6571 - val_loss: 0.9572\n",
      "Epoch 15/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.7983 - loss: 0.4325 - val_accuracy: 0.6571 - val_loss: 1.0334\n",
      "Epoch 16/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.7668 - loss: 0.4303 - val_accuracy: 0.6571 - val_loss: 0.8134\n",
      "Epoch 17/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.8296 - loss: 0.4054 - val_accuracy: 0.6857 - val_loss: 0.7908\n",
      "Epoch 18/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.7859 - loss: 0.4056 - val_accuracy: 0.6429 - val_loss: 0.8390\n",
      "Epoch 19/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - accuracy: 0.8053 - loss: 0.4170 - val_accuracy: 0.6429 - val_loss: 0.8978\n",
      "Epoch 20/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.8279 - loss: 0.4151 - val_accuracy: 0.6714 - val_loss: 1.0396\n",
      "Epoch 21/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.8487 - loss: 0.3707 - val_accuracy: 0.6571 - val_loss: 1.0623\n",
      "Epoch 22/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.8416 - loss: 0.3586 - val_accuracy: 0.6429 - val_loss: 1.1201\n",
      "Epoch 23/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.8610 - loss: 0.3137 - val_accuracy: 0.6000 - val_loss: 0.9375\n",
      "Epoch 24/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.8176 - loss: 0.3713 - val_accuracy: 0.6143 - val_loss: 0.9092\n",
      "Epoch 25/25\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.8948 - loss: 0.3049 - val_accuracy: 0.6429 - val_loss: 0.9678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ab37d99e90>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set,validation_data=test_set,epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325f377-d94a-4cd6-bc46-e979f407c55e",
   "metadata": {},
   "source": [
    "********Part-4 Making single prediction:***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71e8e113-aea3-4089-9def-c79dc9882d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "test_image= image.load_img(r\"C:\\Users\\DELL\\Desktop\\datasets\\test\\dog\\dog2.jpg.jpg\",target_size=(64,64))\n",
    "test_image=image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0]==1:\n",
    "    prediction = \"dog\"\n",
    "else:\n",
    "    prediction = \"cat\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4afc5175-b860-4499-b518-a2e4a734922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f919617d-c670-426d-bcc6-12ca1591c1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "test_image= image.load_img(r\"C:\\Users\\DELL\\Desktop\\datasets\\test\\cat\\cat8.jpg.jpg\",target_size=(64,64))\n",
    "test_image=image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0]==1:\n",
    "    prediction = \"dog\"\n",
    "else:\n",
    "    prediction = \"cat\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f11db7ab-fb87-4a73-ab80-4cc40176a0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd153b-cde2-4cdb-b3e9-1f856d684888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
